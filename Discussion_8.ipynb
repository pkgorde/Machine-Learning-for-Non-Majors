{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion 8: Embedding models and LSTMs with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why all this is important: Natural Language Processing!\n",
    "\n",
    "Natural language processing (NLP) is a computer program's ability to understand human language as it's written and spoken. Utilizing AI as for NLP and Natural Language Understanding has become one of the most important avenues of AI.\n",
    "\n",
    "Different applications of NLP:\n",
    "\n",
    "1. Machine Translation: translating languages from one to another. \n",
    "2. Speech Recognition: the process of converting spoken language into text. \n",
    "3. Sentiment Analysis: the process of detecting opinions through text, positive, negative or neutral\n",
    "4. Named entity recognition (NER): A sub-task of information extraction in NLP that classifies named entities into predefined categories.\n",
    "\n",
    "One big application of NLP, and very much at the peak of ML research today are $\\textbf{Large Language Models}$ such as GPT, LlaMa and others.\n",
    " \n",
    "LLMs are:\n",
    "1. created on extremely large document corpus (eg. all of internet)\n",
    "2. synthesize text inputs, and generate coherent outputs at real time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For starting to work on any of the NLP tasks, there are 2 basic things that you need to implement.\n",
    "\n",
    "1. Using some embedding technique, need to convert sentences into embeddings, which can be inputted into ML models directly. \n",
    "2. Using strong, context-understanding ML models such as LSTMs, transformers, etc. to understand embeddings and creating strong models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We looked at CountVectorizer and Tf-Idf last week. Both of these embeddings are created from the actual data that we are working with. \n",
    "\n",
    "The disadvantage of using a model like this, is that its not broad enough to be able to capture important information at all times.\n",
    "\n",
    "Another very powerful technique to use in this scenario is to utilize powerful pre-trained embedding models provided by packages like gensim and spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (2.10.1)\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.3.2-cp39-cp39-win_amd64.whl.metadata (8.5 kB)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.7.4-cp39-cp39-win_amd64.whl.metadata (27 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from tensorflow) (3.19.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from tensorflow) (69.1.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from tensorflow) (1.62.0)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from tensorflow) (2.10.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from gensim) (1.12.0)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.10-cp39-cp39-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.8-cp39-cp39-win_amd64.whl.metadata (8.6 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp39-cp39-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy)\n",
      "  Downloading thinc-8.2.3-cp39-cp39-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.4.8-cp39-cp39-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.4.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.3.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.9.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting tqdm<5.0.0,>=4.38.0 (from spacy)\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.6.3-py3-none-any.whl.metadata (84 kB)\n",
      "     ---------------------------------------- 0.0/84.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 84.4/84.4 kB 4.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jinja2 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from spacy) (3.1.3)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.16.3 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.16.3-cp39-none-win_amd64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.5.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.0.1)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading blis-0.7.11-cp39-cp39-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading confection-0.1.4-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Collecting click<9.0.0,>=7.1.1 (from typer<0.10.0,>=0.3.0->spacy)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from jinja2->spacy) (2.1.5)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (7.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (3.17.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (3.2.2)\n",
      "Downloading gensim-4.3.2-cp39-cp39-win_amd64.whl (24.0 MB)\n",
      "   ---------------------------------------- 0.0/24.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/24.0 MB 10.0 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 1.3/24.0 MB 13.3 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 2.3/24.0 MB 16.0 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 3.4/24.0 MB 18.0 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 4.7/24.0 MB 19.9 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 5.5/24.0 MB 20.8 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 6.9/24.0 MB 20.9 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 8.3/24.0 MB 23.0 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 10.0/24.0 MB 23.7 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 11.5/24.0 MB 28.5 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 12.0/24.0 MB 27.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 14.0/24.0 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 16.3/24.0 MB 32.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 18.8/24.0 MB 36.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 21.6/24.0 MB 43.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 21.6/24.0 MB 43.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  23.9/24.0 MB 43.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 38.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.0/24.0 MB 32.7 MB/s eta 0:00:00\n",
      "Downloading spacy-3.7.4-cp39-cp39-win_amd64.whl (12.2 MB)\n",
      "   ---------------------------------------- 0.0/12.2 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.5/12.2 MB 48.9 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 2.9/12.2 MB 37.0 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 4.4/12.2 MB 34.7 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 5.9/12.2 MB 34.1 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.5/12.2 MB 34.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 9.1/12.2 MB 34.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.8/12.2 MB 32.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.2/12.2 MB 32.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.2/12.2 MB 31.2 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.8-cp39-cp39-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "   ---------------------------------------- 0.0/181.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 181.6/181.6 kB ? eta 0:00:00\n",
      "Downloading murmurhash-1.0.10-cp39-cp39-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp39-cp39-win_amd64.whl (122 kB)\n",
      "   ---------------------------------------- 0.0/122.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 122.7/122.7 kB ? eta 0:00:00\n",
      "Downloading pydantic-2.6.3-py3-none-any.whl (395 kB)\n",
      "   ---------------------------------------- 0.0/395.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 395.2/395.2 kB 12.0 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.16.3-cp39-none-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------- ----- 1.6/1.9 MB 51.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 30.2 MB/s eta 0:00:00\n",
      "Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "   ---------------------------------------- 0.0/57.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 57.0/57.0 kB ? eta 0:00:00\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp39-cp39-win_amd64.whl (483 kB)\n",
      "   ---------------------------------------- 0.0/483.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 483.8/483.8 kB 29.6 MB/s eta 0:00:00\n",
      "Downloading thinc-8.2.3-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 32.2 MB/s eta 0:00:00\n",
      "Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/45.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 45.9/45.9 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.1/50.1 kB 2.7 MB/s eta 0:00:00\n",
      "Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading blis-0.7.11-cp39-cp39-win_amd64.whl (6.6 MB)\n",
      "   ---------------------------------------- 0.0/6.6 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 1.7/6.6 MB 53.5 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 3.4/6.6 MB 43.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 5.1/6.6 MB 40.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.6/6.6 MB 38.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.6/6.6 MB 32.6 MB/s eta 0:00:00\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "   ---------------------------------------- 0.0/97.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 97.9/97.9 kB ? eta 0:00:00\n",
      "Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/45.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 45.0/45.0 kB 2.3 MB/s eta 0:00:00\n",
      "Downloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: cymem, wasabi, tqdm, spacy-loggers, spacy-legacy, smart-open, pydantic-core, murmurhash, langcodes, cloudpathlib, click, catalogue, blis, annotated-types, typer, srsly, pydantic, preshed, gensim, confection, weasel, thinc, spacy\n",
      "Successfully installed annotated-types-0.6.0 blis-0.7.11 catalogue-2.0.10 click-8.1.7 cloudpathlib-0.16.0 confection-0.1.4 cymem-2.0.8 gensim-4.3.2 langcodes-3.3.0 murmurhash-1.0.10 preshed-3.0.9 pydantic-2.6.3 pydantic-core-2.16.3 smart-open-6.4.0 spacy-3.7.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.3 tqdm-4.66.2 typer-0.9.0 wasabi-1.1.2 weasel-0.3.4\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.8 MB 330.3 kB/s eta 0:00:39\n",
      "     --------------------------------------- 0.1/12.8 MB 525.1 kB/s eta 0:00:25\n",
      "      --------------------------------------- 0.2/12.8 MB 1.4 MB/s eta 0:00:10\n",
      "     - -------------------------------------- 0.5/12.8 MB 2.9 MB/s eta 0:00:05\n",
      "     -- ------------------------------------- 0.9/12.8 MB 4.0 MB/s eta 0:00:03\n",
      "     ---- ----------------------------------- 1.4/12.8 MB 5.0 MB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 1.9/12.8 MB 5.9 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 2.4/12.8 MB 6.7 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 3.0/12.8 MB 7.1 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 3.6/12.8 MB 7.7 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 4.1/12.8 MB 8.3 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 4.8/12.8 MB 8.5 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 5.6/12.8 MB 9.4 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 6.2/12.8 MB 9.6 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 6.7/12.8 MB 9.7 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 7.2/12.8 MB 9.9 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 7.7/12.8 MB 9.9 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 8.1/12.8 MB 9.7 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 8.5/12.8 MB 9.7 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.9/12.8 MB 9.6 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 9.7 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.8/12.8 MB 9.7 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 10.3/12.8 MB 10.4 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 10.7/12.8 MB 10.9 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 11.2/12.8 MB 11.1 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 11.7/12.8 MB 11.1 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 12.1/12.8 MB 11.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.6/12.8 MB 10.9 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 10.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (69.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\premk\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow gensim spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is an example of using gensim to use a pretrained embedding model, GoogleNews-vectors-negative300. for encoding\n",
    "\n",
    "The great thing about this model is that the model is able to convert most words into an embedding of shape (300,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load pretrained word vectors (This path might need to be updated based on where you've saved your model)\n",
    "word_vectors = KeyedVectors.load_word2vec_format(\"C:\\\\Users\\\\premk\\\\Downloads\\\\GoogleNews-vectors-negative300.bin\\\\GoogleNews-vectors-negative300.bin\", binary=True)  \n",
    "\n",
    "# Example of how to get a word vector\n",
    "vector = word_vectors['computer']  # Get the vector for 'computer'\n",
    "print(vector.shape)\n",
    "\n",
    "vector = word_vectors['donut']\n",
    "print(vector.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is also an example of using a small embedding model, directly from spaCy (installed above). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "# Load the small English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize and lemmatize text\n",
    "    doc = nlp(text)\n",
    "    lemmatized = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "    \n",
    "    # Convert tokens to vectors, ignoring those not in our word_vectors model\n",
    "    vectors = [word_vectors[token] for token in lemmatized if token in word_vectors]\n",
    "    \n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros((1, 300))  # Return a zero vector if none of the tokens have vectors in the model\n",
    "    else:\n",
    "        return np.mean(vectors, axis=0)\n",
    "\n",
    "# Example usage\n",
    "text = \"This is an example sentence for processing.\"\n",
    "vector = preprocess_text(text)\n",
    "print(vector.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using existing pretrained embedding models can be very helpful. Please refer to the gensim and spaCy libraries and other implementations and pretrained models to explore how and what to use.\n",
    "\n",
    "Gensim: https://radimrehurek.com/gensim/\n",
    "\n",
    "spaCy: https://spacy.io/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM: the basis of memory implemented in neural networks\n",
    "\n",
    "Long Short-Term Memory (LSTM) networks are a type of recurrent neural network (RNN) that have been specifically designed to address the limitations of traditional RNNs, particularly in handling long-term dependencies. Traditional RNNs struggle to maintain information in their memory for long periods of time, which is a critical aspect when dealing with sequence data like natural language processing, time series analysis, and more. LSTMs overcome this challenge through their unique architecture, which includes memory cells and gates (input, output, and forget gates) that regulate the flow of information.\n",
    "\n",
    "These gates control whether to retain or discard information, making LSTMs capable of learning which data in a sequence is important to keep and which can be thrown away. This ability to remember information for long durations and to effectively manage the vanishing gradient problem makes LSTMs highly advantageous for sequence data, leading to improved performance in tasks like text generation, speech recognition, and more.\n",
    "\n",
    "![alt text](LSTM_cell.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is a diagram of an LSTM (Long Short-Term Memory) cell, which is a building block of LSTM networks, a RNN variant designed to remember information for long periods of time. Here's a step-by-step explanation of how this LSTM cell works:\n",
    "\n",
    "Memory $(C_{t-1})$ and Hidden state $(H_{t-1})$: The cell takes in two pieces of information from the previous time step – the memory $(C_{t-1})$ and the hidden state $(H_{t-1})$. These are the cell's 'memory' of what it has seen in previous steps in the sequence.\n",
    "\n",
    "Input $(X_t)$: Along with the previous memory and hidden state, the cell receives the current input $(X_t)$.\n",
    "\n",
    "Forget Gate $(F_t)$: This gate decides which information is irrelevant and can be discarded from the cell state. It looks at the previous hidden state $(H_{t-1})$ and the current input $(X_t)$ and outputs a number between 0 and 1 for each number in the cell state $(C_{t-1})$. A 1 means “completely keep this” while a 0 means “completely get rid of this”. The sigma (σ) denotes the sigmoid function, which squashes the output to be between 0 and 1.\n",
    "\n",
    "Input Gate $(I_t)$ and Candidate Memory $(~C_t)$: Simultaneously, the input gate decides which new information we're going to store in the cell state. The candidate memory $(~C_t)$, created by applying a tanh function, creates a vector of new candidate values that could be added to the state. The tanh function squashes values to be between -1 and 1.\n",
    "\n",
    "Update Cell State $(C_t)$: The old cell state $(C_{t-1})$ is updated to the new cell state $(C_t)$. The forgotten information is scaled by $F_t$ and then added to the $I_t * ~C_t$ (input gate times the candidate memory) to update the cell state to the new cell state.\n",
    "\n",
    "Output Gate $(O_t)$: The output gate decides what the next hidden state $(H_t)$ should be. It looks at the previous hidden state and the current input and decides which parts of the cell state will be output. Then, it applies tanh to the cell state (to push the values to be between -1 and 1) and multiplies it by the output of the sigmoid gate, so that we only output the parts we decided to.\n",
    "\n",
    "Next Hidden State $(H_t)$: The result is the new hidden state $(H_t)$. This new hidden state and the new cell state $(C_t)$ are then carried over to the next time step.\n",
    "\n",
    "The combination of these gates and memory updates allows the LSTM to effectively capture long-term dependencies and handle the vanishing gradient problem that can occur in standard RNNs. This makes LSTMs particularly well-suited for tasks such as language modeling, machine translation, and speech recognition, where understanding context and maintaining information over time is critical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building an LSTM model in Keras\n",
    "\n",
    "Step-by-step guide on constructing an LSTM model for text classification\n",
    "1. Input Layer: This is the entry point for your data into the model. For an LSTM, the input layer must be specifically formatted to represent sequences. This often involves padding or truncating text sequences to ensure uniformity in sequence length.\n",
    "\n",
    "2. Embedding Layer: This layer transforms the input sequence of word indices into dense vectors of fixed size, typically much more compact than the one-hot encoding representations. Using pretrained embeddings like GloVe or Word2Vec is optional but can significantly boost the model's performance by leveraging prior knowledge of word associations.\n",
    "\n",
    "3. LSTM Layer(s): Here, one or more LSTM layers are added to process the sequence of word embeddings. The LSTM layers learn to identify and utilize long-term dependencies in the data, essential for understanding the context and semantics in text classification tasks.\n",
    "\n",
    "4. Dense Layer(s) for Classification: After processing the sequences with LSTM layers, the output is flattened or pooled and fed into one or more dense layers. These layers serve to map the learned sequence representations to the desired output format, such as the classes in a classification task.\n",
    "\n",
    "5. Compilation (loss function, optimizer): Finally, the model is compiled, specifying a loss function and an optimizer. For a classification problem, the loss function is often categorical crossentropy, while the optimizer could be Adam, RMSprop, or SGD. This step also typically includes specifying any metrics to monitor during training, such as accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM implementation on Twitter Sentiment Analysis dataset.\n",
    "\n",
    "data adapted from Kaggle: https://huggingface.co/datasets/carblacac/twitter-sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentiment          id                          time     query  \\\n",
      "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
      "1          0  1467813992  Mon Apr 06 22:20:38 PDT 2009  NO_QUERY   \n",
      "2          0  1467814883  Mon Apr 06 22:20:52 PDT 2009  NO_QUERY   \n",
      "3          0  1467858869  Mon Apr 06 22:32:20 PDT 2009  NO_QUERY   \n",
      "4          0  1467872181  Mon Apr 06 22:35:50 PDT 2009  NO_QUERY   \n",
      "\n",
      "          username                                              tweet  \n",
      "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
      "1       swinspeedx  one of my friend called me, and asked to meet ...  \n",
      "2            gagoo                             im sad now  Miss.Lilly  \n",
      "3       Jaderade14   is watching the hill . . .and its making me sad   \n",
      "4           admdrw  @charlietm I know right. I dunno what is going...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "skip = lambda i: i > 0 and np.random.rand() > 0.01\n",
    "\n",
    "df = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding_errors='ignore', names=['sentiment', 'id', 'time', 'query', 'username', 'tweet'], skiprows=skip)\n",
    "\n",
    "# Display the first few rows to confirm it's loaded correctly\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove hashtags and mentions\n",
    "    text = re.sub(r'(@\\w+|#\\w+)', '', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Remove numbers and special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Lowercase all text\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 100, 100)          1937800   \n",
      "                                                                 \n",
      " spatial_dropout1d_2 (Spatia  (None, 100, 100)         0         \n",
      " lDropout1D)                                                     \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 100, 128)          117248    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 100, 128)          0         \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,108,681\n",
      "Trainable params: 2,108,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "200/200 [==============================] - 253s 1s/step - loss: -67.1775 - accuracy: 0.0013 - val_loss: -180.3834 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "200/200 [==============================] - 240s 1s/step - loss: -363.3472 - accuracy: 0.0000e+00 - val_loss: -635.5421 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/5\n",
      "200/200 [==============================] - 240s 1s/step - loss: -947.1004 - accuracy: 0.0000e+00 - val_loss: -1385.1647 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/5\n",
      "200/200 [==============================] - 244s 1s/step - loss: -1788.1976 - accuracy: 0.0000e+00 - val_loss: -2402.1101 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/5\n",
      "200/200 [==============================] - 241s 1s/step - loss: -2856.4087 - accuracy: 0.0000e+00 - val_loss: -3663.6448 - val_accuracy: 0.0000e+00\n",
      "100/100 [==============================] - 11s 112ms/step - loss: -3663.6458 - accuracy: 0.0000e+00\n",
      "Test Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SpatialDropout1D, LSTM, Dense, Dropout\n",
    "\n",
    "# Preprocess the text data\n",
    "# This function would need to be defined in your code, adapting for tweet-specific content\n",
    "X = df['tweet'].apply(preprocess_text).tolist()\n",
    "y = df['sentiment'].values\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "max_length = 100  # Adjust based on your dataset\n",
    "X_pad = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the LSTM model architecture\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=100, input_length=max_length),\n",
    "    SpatialDropout1D(0.2),\n",
    "    LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True),  # First LSTM layer\n",
    "    Dropout(0.2),\n",
    "    LSTM(64, dropout=0.2, recurrent_dropout=0.2),  # Second LSTM layer\n",
    "    Dense(64, activation='relu'),  # Additional Dense layer to increase model capacity\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile and train the model\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# model.summary()\n",
    "# history = model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3198, 100)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SpatialDropout1D, LSTM, Dense, Dropout\n",
    "\n",
    "# Preprocess the text data\n",
    "# This function would need to be defined in your code, adapting for tweet-specific content\n",
    "X = df['tweet'].apply(preprocess_text).tolist()\n",
    "y = df['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and pad sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "max_length = 100  # Adjust based on your dataset\n",
    "X_pad = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('one of my friend called me and asked to meet with her at mid valley todaybut ive no time sigh',\n",
       " [54,\n",
       "  13,\n",
       "  5,\n",
       "  271,\n",
       "  485,\n",
       "  14,\n",
       "  6,\n",
       "  819,\n",
       "  2,\n",
       "  410,\n",
       "  22,\n",
       "  100,\n",
       "  24,\n",
       "  3619,\n",
       "  4737,\n",
       "  7071,\n",
       "  124,\n",
       "  37,\n",
       "  51,\n",
       "  737])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1], sequences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model architecture\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=100, input_length=max_length),\n",
    "    SpatialDropout1D(0.2),\n",
    "    LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True),  # First LSTM layer\n",
    "    Dropout(0.2),\n",
    "    LSTM(64, dropout=0.2, recurrent_dropout=0.2),  # Second LSTM layer\n",
    "    Dense(64, activation='relu'),  # Additional Dense layer to increase model capacity\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
